# Use middle state. Policy on state 0 differs at timesteps
# Based on mid-s, but scales the reward to strength difference, and shorten horizon
# Removed the last state
# 
# Best policy : (0,1,7),(2,1,7),(0,1,7),...,(2,1,7) 7*20=140
# Suboptimal: (0,0,10),(1,1,3),(1,1,3), ... 3*19+10=67
#             (0,0,10),(1,0,0),(0,0,10),(1,0,0), ... . 10*10=100
#             (0,1,7),(2,0,3),(2,0,3),... 7+3*19=64
# |S|, |A|, initial state distribution, H, P, r
# state space: 0,1,...,|S|-1
# action space: 0,1,...,|A|-1
# initial state distribution: prob0,prob1,...,prob(|S|-1)
# P: each row "s,a,prob0,prob1,...,prob(|S|-1)"
# r: each row "s,a,r"
# Lines starting with "#" or empty lines will be omitted

# |S|
3

# |A|
2

# H
20

# initial state distribution
1,0,0

# P, |S||A| rows
0,0,0,1,0
0,1,0,0,1
1,0,1,0,0
1,1,0,1,0
2,0,0,0,1
2,1,1,0,0

# r, |S||A| rows
0,0,10
0,1,7
1,0,0
1,1,3
2,0,3
2,1,7