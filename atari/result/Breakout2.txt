python run_dt_atari.py --seed 123 --context_length 30 --epochs 5 --model_type reward_conditioned --num_steps 500000 --num_buffers 50 --game Breakout --batch_size 128
Namespace(batch_size=128, context_length=30, data_dir_prefix='./dqn_replay/', epochs=5, game='Breakout', log_level='WARNING', model_type='reward_conditioned', num_buffers=50, num_steps=500000, seed=123, trajectories_per_buffer=10)
max rtg is 98
max timestep is 2654
GPU available.
device=0
target return: 90, eval return: 63
target return: 90, eval return: 56
target return: 90, eval return: 57
target return: 90, eval return: 57
target return: 90, eval return: 54
python run_dt_atari.py --seed 231 --context_length 30 --epochs 5 --model_type reward_conditioned --num_steps 500000 --num_buffers 50 --game Breakout --batch_size 128
Namespace(batch_size=128, context_length=30, data_dir_prefix='./dqn_replay/', epochs=5, game='Breakout', log_level='WARNING', model_type='reward_conditioned', num_buffers=50, num_steps=500000, seed=231, trajectories_per_buffer=10)
max rtg is 98
max timestep is 3165
GPU available.
device=0
target return: 90, eval return: 36
target return: 90, eval return: 44
target return: 90, eval return: 39
target return: 90, eval return: 47
target return: 90, eval return: 74
python run_dt_atari.py --seed 312 --context_length 30 --epochs 5 --model_type reward_conditioned --num_steps 500000 --num_buffers 50 --game Breakout --batch_size 128
Namespace(batch_size=128, context_length=30, data_dir_prefix='./dqn_replay/', epochs=5, game='Breakout', log_level='WARNING', model_type='reward_conditioned', num_buffers=50, num_steps=500000, seed=312, trajectories_per_buffer=10)
max rtg is 98
max timestep is 3476
GPU available.
device=0
target return: 90, eval return: 33
target return: 90, eval return: 33
target return: 90, eval return: 65
target return: 90, eval return: 31
target return: 90, eval return: 41


python run_dt_atari.py --seed 123 --context_length 30 --epochs 5 --model_type reward_conditioned --num_steps 500000 --num_buffers 50 --game Breakout --batch_size 128
Namespace(batch_size=128, context_length=30, data_dir_prefix='./dqn_replay/', epochs=5, game='Breakout', log_level='WARNING', model_type='reward_conditioned', num_buffers=50, num_steps=500000, seed=123, trajectories_per_buffer=10)
max rtg is 98
max timestep is 2654
GPU available.
device=0
target return: 90, eval return: 26
target return: 90, eval return: 61
target return: 90, eval return: 39
target return: 90, eval return: 28
target return: 90, eval return: 55
python run_dt_atari.py --seed 231 --context_length 30 --epochs 5 --model_type reward_conditioned --num_steps 500000 --num_buffers 50 --game Breakout --batch_size 128
Namespace(batch_size=128, context_length=30, data_dir_prefix='./dqn_replay/', epochs=5, game='Breakout', log_level='WARNING', model_type='reward_conditioned', num_buffers=50, num_steps=500000, seed=231, trajectories_per_buffer=10)
max rtg is 98
max timestep is 3165
GPU available.
device=0
target return: 90, eval return: 27
target return: 90, eval return: 61
target return: 90, eval return: 56
target return: 90, eval return: 24
target return: 90, eval return: 30
python run_dt_atari.py --seed 312 --context_length 30 --epochs 5 --model_type reward_conditioned --num_steps 500000 --num_buffers 50 --game Breakout --batch_size 128
Namespace(batch_size=128, context_length=30, data_dir_prefix='./dqn_replay/', epochs=5, game='Breakout', log_level='WARNING', model_type='reward_conditioned', num_buffers=50, num_steps=500000, seed=312, trajectories_per_buffer=10)
max rtg is 98
max timestep is 3476
GPU available.
device=0
target return: 90, eval return: 34
target return: 90, eval return: 56
target return: 90, eval return: 69
target return: 90, eval return: 41
target return: 90, eval return: 38